# In this Week
![image](https://github.com/JinnyKo/NLP-Specialization-Coursera/assets/93627969/90c40b4a-01ea-41f2-8395-500682618715)

# Basic Word Representations (단어 표현 방법) 
- **Integers**
 각 단어에 고유한 정수를 할당하는 방법.예를 들어 "happy"라는 단어에 621이라는 숫자를 할당한다. 하지만 단어 사이에 수학적 관계가 없음에도 불구하고 숫자 크기에 의한 임의의 순서나 관계가 생긴다.

- **One-hot vectors**
 단어를 벡터로 표현하는데, 벡터의 크기는 어휘(Vocabulary)의 크기와 같고, 해당 단어의 인덱스 위치에만 1을 두고 나머지는 모두 0으로 채우는 방법이다. 예를들어 "happy"라는 단어에 대한 원-핫 벡터는 "happy"의 인덱스에 해당하는 위치에만 1을 두고, 나머지는 0으로 이루어져 있다.
=> **고차원의 희소한 벡터**
ex) 예를 들어, 단어 "사과"를 표현할 때, 단어 집합의 크기가 10,000이라면 "사과"에 해당하는 인덱스 위치만 1이고 나머지는 0인 10,000차원의 벡터가 만들어진다.

- **Word embedding**
각 단어를 저차원의 연속적인 값으로 이루어진 벡터로 표현하는 방법. 단어 임베딩은 단어 간의 의미적 관계를 포착하고, 단어 사이의 유사성을 수치적으로 표현할 수 있다.
=> **저차원의 밀집된 벡터**
ex) 예를 들어, "사과"라는 단어를 100차원의 벡터로 표현할 수 있다. 이 벡터는 각 차원이 해당 단어의 특정 의미적 특징을 나타낸다. [0.2,0.5,−0.1,...,0.3(n=100)]





